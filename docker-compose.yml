services:
  python_app:
    build:
      context: .
      dockerfile: Dockerfile
    image: finnhub_client:latest
    volumes:
      - .:/app
    env_file:
      - .env
    environment: 
      - PYTHONUNBUFFERED=1
      - KAFKA_BOOTSTRAP_SERVERS=broker:29092
    depends_on:
      broker:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    restart: always
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; socket.socket().connect(('broker', 29092))"]
      interval: 5s
      timeout: 5s
      retries: 5

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.2
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - kafka-network
    healthcheck:
      test: echo srvr | nc localhost 2181 || exit 1
      interval: 5s
      timeout: 5s
      retries: 5

  broker:
    image: confluentinc/cp-kafka:7.5.2
    hostname: broker
    container_name: broker
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_CREATE_TOPICS: "finnhub.trades:1:1"
      KAFKA_NUM_PARTITIONS: 1
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"
      KAFKA_LOG_RETENTION_HOURS: 24
      KAFKA_LOG_RETENTION_BYTES: "1073741824"
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.2
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      broker:
        condition: service_healthy
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker:29092'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'
    networks:
      - kafka-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 5s
      timeout: 5s
      retries: 5

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "8080:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=broker:29092
      - KAFKA_CLUSTERS_0_ZOOKEEPER=zookeeper:2181
      - KAFKA_CLUSTERS_0_SCHEMAREGISTRY=http://schema-registry:8081
    depends_on:
      broker:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    networks:
      - kafka-network

  cassandra:
    image: bitnami/cassandra:4.1
    container_name: cassandra
    ports:
      - '7000:7000'
      - '9042:9042'
    volumes:
      - 'cassandra_market_data:/bitnami'
      - './src/casandra/casandra-setup.cql:/docker-entrypoint-initdb.d/schema.cql'
    environment:
      - CASSANDRA_SEEDS=cassandra
      - CASSANDRA_PASSWORD_SEEDER=yes
      - CASSANDRA_PASSWORD=${CASSANDRA_PASSWORD}
      - CASSANDRA_INIT_DB_FILE=/docker-entrypoint-initdb.d/schema.cql
      - CASSANDRA_CLUSTER_NAME=${CASSANDRA_CLUSTER_NAME}
      - CASSANDRA_DC=${CASSANDRA_DC}
      - CASSANDRA_RACK=${CASSANDRA_RACK}
      - CASSANDRA_BROADCAST_ADDRESS=${CASSANDRA_BROADCAST_ADDRESS}
      - CASSANDRA_LISTEN_ADDRESS=${CASSANDRA_LISTEN_ADDRESS}
      - CASSANDRA_USER=${CASSANDRA_USER}
    networks:
      kafka-network:
        ipv4_address: 172.19.0.10
    healthcheck:
      test: ["CMD-SHELL", "cqlsh -u cassandra -p cassandra -e 'describe keyspaces;'"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 40s

  cassandra-web:
    image: markusgulden/cassandra-web:0.5.0
    container_name: cassandra-web
    depends_on:
      cassandra:
        condition: service_healthy
    environment:
      - CASSANDRA_HOST_IPS=${CASSANDRA_BROADCAST_ADDRESS}
      - CASSANDRA_PORT=${CASSANDRA_PORT}
      - CASSANDRA_USERNAME=${CASSANDRA_USER}
      - CASSANDRA_PASSWORD=${CASSANDRA_PASSWORD}
    ports:
      - "3002:3000"
    networks:
      - kafka-network
    restart: unless-stopped
    extra_hosts:
      - "cassandra:172.19.0.10"

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./grafana/dashboards:/var/lib/grafana/dashboards
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin}
      - GF_USERS_ALLOW_SIGN_UP=false
    networks:
      - kafka-network
    depends_on:
      cassandra:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  spark-master:
    build:
      context: .
      dockerfile: src/processor/Dockerfile
    container_name: spark-master
    command: bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8090:8080"
      - "7077:7077"
    volumes:
      - ./src/processor/src:/opt/bitnami/spark/scripts
    env_file:
      - .env
    environment:
      - SPARK_MODE=master
      - KAFKA_BOOTSTRAP_SERVERS=broker:29092
      - CASSANDRA_HOST=cassandra
    networks:
      - kafka-network
    depends_on:
      broker:
        condition: service_healthy
      cassandra:
        condition: service_healthy

  spark-worker1:
    build:
      context: .
      dockerfile: src/processor/Dockerfile
    container_name: spark-worker1
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 4g
      SPARK_MASTER_URL: spark://spark-master:7077
      KAFKA_BOOTSTRAP_SERVERS: broker:29092
      CASSANDRA_HOST: cassandra
    volumes:
      - ./src/processor/src:/opt/bitnami/spark/scripts
    env_file:
      - .env
    networks:
      - kafka-network
    depends_on:
      - spark-master

  spark-worker2:
    build:
      context: .
      dockerfile: src/processor/Dockerfile
    container_name: spark-worker2
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 4g
      SPARK_MASTER_URL: spark://spark-master:7077
      KAFKA_BOOTSTRAP_SERVERS: broker:29092
      CASSANDRA_HOST: cassandra
    volumes:
      - ./src/processor/src:/opt/bitnami/spark/scripts
    env_file:
      - .env
    networks:
      - kafka-network
    depends_on:
      - spark-master

  spark-streaming:
    build:
      context: .
      dockerfile: src/processor/Dockerfile
    container_name: spark-streaming
    command: >
      /opt/bitnami/spark/bin/spark-submit
      --master spark://spark-master:7077
      --name "FinnhubStreamProcessor"
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-avro_2.12:3.5.0,com.datastax.spark:spark-cassandra-connector_2.12:3.5.0
      --conf spark.sql.avro.compression.codec=snappy
      --conf spark.cassandra.connection.host=cassandra
      --conf spark.cassandra.connection.port=9042
      --conf spark.cassandra.auth.username=cassandra
      --conf spark.cassandra.auth.password=cassandra
      /opt/bitnami/spark/scripts/stream_processor.py
    volumes:
      - ./src/processor/src:/opt/bitnami/spark/scripts
    env_file:
      - .env
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=broker:29092
      - CASSANDRA_HOST=cassandra
    networks:
      - kafka-network
    depends_on:
      - spark-master
      - broker
      - cassandra
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
    healthcheck:
      test: ["CMD-SHELL", "ps aux | grep stream_processor.py | grep -v grep || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  cassandra_market_data:
    name: cassandra_market_data
  kafka_data:
    name: kafka_data
  grafana_data:
    name: grafana_data

networks:
  kafka-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.19.0.0/16
  